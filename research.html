<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Research | Manda Fischer</title>

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&family=Montserrat:wght@300;400;600;700&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="styles.css">
</head>

<body>

<div class="page">

  <aside class="sidebar">

    <div class="avatar-wrap">
      <img class="avatar" src="assets/Website_Feb2026_2.jpg" alt="Headshot of Manda Fischer">
    </div>
    
    <h1 class="name">
      Manda Fischer, <span class="degree">Ph.D.</span>
    </h1>

    <p class="meta">Postdoctoral Researcher</p>
    <p class="meta">Department of Psychology</p>
    <p class="meta">University of Toronto</p>

    <div class="icon-row">

      <a class="icon-btn" href="mailto:manda.fischer@utoronto.ca" aria-label="Email">
        <svg viewBox="0 0 24 24" aria-hidden="true">
          <path d="M20 4H4a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2V6a2 2 0 0 0-2-2Zm0 4-8 5-8-5V6l8 5 8-5v2Z"/>
        </svg>
      </a>

      <a class="icon-btn" href="https://scholar.google.com/citations?user=4h2fU-MAAAAJ&hl=en" target="_blank" rel="noopener" aria-label="Google Scholar">
        <svg viewBox="0 0 24 24" aria-hidden="true">
          <path d="M12 3 1 9l11 6 9-4.91V17h2V9L12 3Zm0 10L4.74 9 12 5l7.26 4L12 13Zm-6 3.5V19c0 1.66 2.69 3 6 3s6-1.34 6-3v-2.5l-6 3-6-3Z"/>
        </svg>
      </a>

      <!-- LinkedIn -->
      <a class="icon-btn"
         href="https://ca.linkedin.com/in/manda-fischer"
         target="_blank"
         rel="noopener"
         aria-label="LinkedIn">
        <svg viewBox="0 0 24 24" aria-hidden="true">
          <path d="M20.45 20.45h-3.55v-5.6c0-1.33-.03-3.05-1.86-3.05-1.86 0-2.15 1.45-2.15 2.95v5.7H9.34V9h3.41v1.56h.05c.48-.9 1.65-1.86 3.39-1.86 3.62 0 4.29 2.38 4.29 5.47v6.28ZM5.34 7.43a2.06 2.06 0 1 1 0-4.12 2.06 2.06 0 0 1 0 4.12ZM7.11 20.45H3.56V9h3.55v11.45Z"/>
        </svg>
      </a>

    </div>

  </aside>

  <main class="main">

    <header class="topnav">
      <nav>
        <a href="index.html">About</a>
        <a class="active" href="research.html">Research</a>
        <a href="funding.html">Funding</a>
        <a href="teaching.html">Teaching</a>
        <a href="cv.html">CV</a>
      </nav>
    </header>

    <section class="content research-content">

      <h2 class="section-title">Research</h2>
      
      <p>
        How does the brain produce stable perception from evidence that is partial, shifting, and sometimes conflicting?
        Understanding how internal representations organize variable input is central to cognitive neuroscience.
        I investigate, across sensory systems, temporal scales, and conditions of uncertainty, how representational structure constrains perceptual inference.
      </p>
      
      <p>
        To address this question, I integrate behavioural manipulations with neurophysiological (EEG) and statistical modeling to quantify representational fidelity,
        dissociate temporal stages of inference, and test reweighting under distinct forms of sensory degradation.
        My work extends this framework to domains such as music and speech to test whether a unified set of  principles of perceptual inference generalize across complex contexts and whether they can be leveraged to mitigate age- and hearing-related sensory decline.
      </p>
      
      <p>
        <strong>My long-term goal is to establish a unified, amodal account of perceptual inference grounded in shared principles of representational structure and aligned across sensory domains.</strong>
      </p>
    
      <!-- 1 -->
      <div class="research-block" id="research1">
        <div class="research-text">
          <h3 class="theme-title">1. Determining When Learning Shapes Perception</h3>
          <p class="theme-hook">
            <em>When does acquired knowledge guide real-time inference and when does it remain silent?</em>
          </p>
      
          <p>
            Learning does not guarantee its expression. I show that experience can leave robust neural traces that remain behaviourally silent, separating learning from its deployment in perception.
          </p>
      
          <p>
            This dissociation raises two key questions. Is the transition from learning to expression gradual or thresholded? What mechanisms allow silent traces to be reawakened?
          </p>
      
          <p>
            To answer them, my research combines controlled behavioural paradigms with high-temporal-resolution EEG to track memory formation and its influence on perception. By manipulating attention, goals, awareness, and context, I identify when prior experience shapes perception and how silent representations are reactivated.
          </p>
      
          <!-- ================= PREVIOUS VERSION (COMMENTED OUT) =================
          <p>
            What fascinates me is that learning does not guarantee expression. In studying contextual cueing 
            and memory-guided attention, my research demonstrates neural signatures of learning
            in the absence of behavioural benefit and reveals latent memory traces that do not automatically influence performance.
            This dissociation raises a central question: what determines whether stored information is merely 
            available or is accessible when needed? I examine how attention during encoding, 
            task instructions, awareness, and cognitive load regulate whether learned information influences perceptual decision-making.
            Rather than treating memory-guided perception as binary, my work characterizes it as a 
            dynamic interaction between stored knowledge and current task demands.
          </p>
          -->
          
        </div>
      
        <figure class="research-image">
          <img src="assets/research1.png" alt="Memory-Guided Perception figure">
        </figure>
      
      </div>


      <!-- 2 -->
      <div class="research-block reverse" id="research2">
        <div class="research-text">
          <h3 class="theme-title">2. Dissociating Prediction from Postdiction in Perceptual Inference</h3>
          <p class="theme-hook">
            <em>Do predictive and postdictive influences reflect the same process, or fundamentally different forms of inference?</em>
          </p>
      
          <p>
            Context shapes perception both before input arrives and after additional information becomes available. 
            In my work, I test whether predictive and postdictive influences reflect shared or distinct mechanisms 
            and demonstrate that they diverge in their temporal dynamics and behavioural consequences.
          </p>
      
          <p>
            A genuine dissociation implies that anticipation and revision operate under different informational constraints 
            and are differentially sensitive to timing and uncertainty. I therefore manipulate sensory reliability and temporal structure 
            across audiovisual contexts to determine how contextual information is reweighted as evidence unfolds.
          </p>
      
          <p>
            Clarifying these stage-specific constraints is central to building a unified account of how perceptual inference unfolds as evidence becomes available.
          </p>
      
          <!-- ================= PREVIOUS VERSION (COMMENTED OUT) =================
          <p>
            Perception unfolds over time and integrates information across senses. My work examines how contextual 
            information shapes perceptual judgments at distinct temporal stages. In some cases, prior knowledge generates 
            predictions that bias interpretation before sensory input arrives; in others, later input retrospectively reshapes what was perceived.
            I test whether predictive and postdictive influences rely on shared or dissociable mechanisms, 
            and how the system flexibly reweights these influences, depending on 
            timing, uncertainty, and task demands.
          </p>
          -->
      
        </div>
      
        <figure class="research-image">
          <img src="assets/research2.png" alt="Multisensory Inference figure">
        </figure>
      
      </div>


     <!-- 3 -->
    <div class="research-block" id="research3">
      <div class="research-text">
        <h3 class="theme-title">3. Aligning Feature Structure Across Modalities</h3>
        <p class="theme-hook">
          <em>Can a common feature space be established to test whether perceptual inference principles generalize across senses?</em>
        </p>
    
        <p>
          A principled comparison of perceptual inference across modalities rests on expressing sensory information within a shared feature space.
          In my work, I extend circular feature-space methods to align auditory and visual dimensions within a common continuous format,
          enabling direct comparison of precision and bias across senses.
        </p>
    
        <p>
          This alignment strengthens cross-modal tests of perceptual inference by ensuring that observed differences reflect genuine modality-specific processes.
          Establishing a common feature space provides a principled foundation for building a unified account of perceptual inference across sensory domains.
        </p>
    
        <!-- ================= PREVIOUS VERSION (COMMENTED OUT) =================
        <p>
          To compare how information constrains perception across modalities, it must be characterized within a 
          shared continuous feature space. In vision, circular feature spaces enable estimation of 
          response precision and systematic bias along defined stimulus dimensions. 
          Comparable approaches are now emerging in audition research.
          In this line of work, I extend circular-space methods to align auditory and visual feature dimensions 
          within a common continuous format, allowing for direct cross-modal comparison 
          and for systematic tests of multisensory integration.
        </p>
        -->
    
      </div>
    
      <figure class="research-image">
        <img src="assets/research3.png" alt="Representational Structure figure">
      </figure>
    
    </div>

      <!-- 4 -->
      <div class="research-block reverse" id="research4">
        <div class="research-text">
          <h3 class="theme-title">4. Using Music to Test Principles of Perceptual Inference</h3>
          <p class="theme-hook">
            <em>How do acoustic cues and musical structure jointly constrain perceptual inference?</em>
          </p>
      
          <p>
            Music provides a powerful testbed for probing how sensory features and learned structure jointly shape perception. I dissociate bottom-up acoustic cues from experience-driven expectations and show that perceptual inference in complex auditory scenes reflects systematic cue weighting rather than dominance by either source alone. By examining how these influences interact, I test whether core principles of cue integration extend to richly organized auditory input.
          </p>
      
          <!-- ================= PREVIOUS VERSION (COMMENTED OUT) =================
          <h3 class="theme-title">4. Music as a Window into Structure and Inference</h3>
          <p class="theme-hook"><em>How do internal expectations and external cues interact to organize complex musical scenes?</em></p>
          <p>
            Music provides a richly structured yet dynamic environment for probing perceptual organization. 
            I use orchestral materials to test how acoustic and score-based factors shape auditory grouping and perceptual prominence. 
            I also examine how experience (musicians and non-musicians) modulates the weighting of these cues. 
            In this way, music exposes how naturalistic structured input and experience jointly constrain 
            perceptual inference.
          </p>
          -->
        </div>
      
        <figure class="research-image">
          <img src="assets/research4.png" alt="Music and Perceptual Organization figure">
        </figure>
      
      </div>

      <!-- 5 -->
      <div class="research-block" id="research5">
        <div class="research-text">
          <h3 class="theme-title">5. Adapting and Strengthening Perceptual Inference Under Sensory Degradation</h3>
          <p class="theme-hook">
            <em>How can perception be supported when sensory input becomes unreliable?</em>
          </p>
      
          <p>
            When sensory input becomes unreliable, successful perception is constrained and must rely on internally stored knowledge to resolve ambiguity. 
            In my work, I manipulate stimulus reliability to examine how the influence of prior knowledge changes as the signal degrades.
          </p>
      
          <p>
            In parallel, I investigate voice familiarity as a form of stored knowledge that can be leveraged to improve speech intelligibility. 
            I show that familiarity benefits depend on attention and cognitive resources, vary across individuals, and can be strengthened through targeted exposure.
          </p>
      
          <p>
            By clarifying how prior knowledge supports perception under degraded conditions, this work provides a foundation for strengthening communication in aging and hearing-related decline.
          </p>
      
          <!-- ================= PREVIOUS VERSION (COMMENTED OUT) =================
          <h3 class="theme-title">5. Perceptual Inference Under Sensory Degradation</h3>
          <p class="theme-hook"><em>How is stored knowledge leveraged for perception when sensory input becomes unreliable?</em></p>
          <p>
            When sensory input becomes unreliable, successful perception is constrained and must rely on internally stored knowledge 
            to resolve ambiguity. I examine how voice familiarity can improve speech intelligibility 
            in noisy environments, particularly for older adults and for individuals with hearing loss. 
            My work uses sensory degradation both as a clinically relevant challenge 
            and as a principled test of how internal knowledge compensates for reduced signal reliability.
          </p>
          -->
        
        </div>
      
        <figure class="research-image">
          <img src="assets/research5.png" alt="Real-World Applications figure">
        </figure>
      
      </div>

    </section>

  </main>

</div>

</body>
</html>
